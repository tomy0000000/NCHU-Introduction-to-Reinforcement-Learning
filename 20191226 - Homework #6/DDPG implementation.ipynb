{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from: [sweetice/Deep-reinforcement-learning-with-pytorch](https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch/tree/master/Char05%20DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules and Configuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from itertools import count\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Configs & Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI gym environment name, # ['BipedalWalker-v2', 'Pendulum-v0'] or any continuous environment\n",
    "# Note that DDPG is feasible about hyper-parameters.\n",
    "# You should fine-tuning if you change to another environment.\n",
    "ENV_NAME = \"Pendulum-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env = gym.make(ENV_NAME).unwrapped\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "min_Val = torch.tensor(1e-7).float().to(device) # min value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = 50000 # replay buffer size\n",
    "TAU = 0.005 # target smoothing coefficient\n",
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99 # discounted factor\n",
    "BATCH_SIZE = 64 # mini batch size\n",
    "UPDATE_ITERATION = 10\n",
    "DIRECTORY = \"exp_{}/\".format(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     31,
     46,
     59
    ]
   },
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size=CAPACITY):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x))\n",
    "        return x\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400 , 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = F.relu(self.l1(torch.cat([x, u], 1)))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), LEARNING_RATE)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), LEARNING_RATE)\n",
    "        self.replay_buffer = Replay_buffer()\n",
    "        self.writer = SummaryWriter(DIRECTORY)\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        for it in range(UPDATE_ITERATION):\n",
    "            # Sample replay buffer\n",
    "            x, y, u, r, d = self.replay_buffer.sample(BATCH_SIZE)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target_Q = reward + ((1 - done) * GAMMA * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimate\n",
    "            current_Q = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "            self.writer.add_scalar('Loss/critic_loss', critic_loss, global_step=self.num_critic_update_iteration)\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "            self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "            self.num_actor_update_iteration += 1\n",
    "            self.num_critic_update_iteration += 1\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.actor.state_dict(), DIRECTORY + 'actor.pth')\n",
    "        torch.save(self.critic.state_dict(), DIRECTORY + 'critic.pth')\n",
    "        # print(\"====================================\")\n",
    "        # print(\"Model has been saved...\")\n",
    "        # print(\"====================================\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_state_dict(torch.load(DIRECTORY + 'actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(DIRECTORY + 'critic.pth'))\n",
    "        print(\"====================================\")\n",
    "        print(\"model has been loaded...\")\n",
    "        print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER = True # show UI or not\n",
    "LOG_INTERVAL = 50\n",
    "LOAD = False # load model\n",
    "EXPLORATION_NOISE = 0.1\n",
    "MAX_EPISODE = 1500 # num of games\n",
    "MAX_LENGTH_OF_TRAJECTORY = 2000 # num of games\n",
    "PRINT_LOG = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Collection Experience...\n",
      "====================================\n",
      "Ep_i \t0, the ep_r is \t-11462.75, the step is \t2000\n",
      "Ep_i \t5, the ep_r is \t-12378.22, the step is \t2000\n",
      "Ep_i \t10, the ep_r is \t-13139.87, the step is \t2000\n",
      "Ep_i \t15, the ep_r is \t-12236.66, the step is \t2000\n",
      "Ep_i \t20, the ep_r is \t-11595.00, the step is \t2000\n",
      "Ep_i \t25, the ep_r is \t-19205.06, the step is \t2000\n",
      "Ep_i \t30, the ep_r is \t-17465.96, the step is \t2000\n",
      "Ep_i \t35, the ep_r is \t-15953.73, the step is \t2000\n",
      "Ep_i \t40, the ep_r is \t-16286.34, the step is \t2000\n",
      "Ep_i \t45, the ep_r is \t-15965.87, the step is \t2000\n",
      "Ep_i \t50, the ep_r is \t-13854.60, the step is \t2000\n",
      "Ep_i \t55, the ep_r is \t-13083.17, the step is \t2000\n",
      "Ep_i \t60, the ep_r is \t-12491.64, the step is \t2000\n",
      "Ep_i \t65, the ep_r is \t-12293.70, the step is \t2000\n",
      "Ep_i \t70, the ep_r is \t-11611.28, the step is \t2000\n",
      "Ep_i \t75, the ep_r is \t-16273.28, the step is \t2000\n",
      "Ep_i \t80, the ep_r is \t-13507.06, the step is \t2000\n",
      "Ep_i \t85, the ep_r is \t-14492.36, the step is \t2000\n",
      "Ep_i \t90, the ep_r is \t-13787.16, the step is \t2000\n",
      "Ep_i \t95, the ep_r is \t-14260.83, the step is \t2000\n",
      "Ep_i \t100, the ep_r is \t-13622.22, the step is \t2000\n",
      "Ep_i \t105, the ep_r is \t-11718.35, the step is \t2000\n",
      "Ep_i \t110, the ep_r is \t-15885.79, the step is \t2000\n",
      "Ep_i \t115, the ep_r is \t-11305.74, the step is \t2000\n",
      "Ep_i \t120, the ep_r is \t-10312.44, the step is \t2000\n",
      "Ep_i \t125, the ep_r is \t-14630.14, the step is \t2000\n",
      "Ep_i \t130, the ep_r is \t-13483.97, the step is \t2000\n",
      "Ep_i \t135, the ep_r is \t-15541.83, the step is \t2000\n",
      "Ep_i \t140, the ep_r is \t-15324.31, the step is \t2000\n",
      "Ep_i \t145, the ep_r is \t-12923.26, the step is \t2000\n",
      "Ep_i \t150, the ep_r is \t-12193.84, the step is \t2000\n",
      "Ep_i \t155, the ep_r is \t-13533.15, the step is \t2000\n",
      "Ep_i \t160, the ep_r is \t-13665.46, the step is \t2000\n",
      "Ep_i \t165, the ep_r is \t-14420.43, the step is \t2000\n",
      "Ep_i \t170, the ep_r is \t-10196.69, the step is \t2000\n",
      "Ep_i \t175, the ep_r is \t-8680.67, the step is \t2000\n",
      "Ep_i \t180, the ep_r is \t-15038.51, the step is \t2000\n",
      "Ep_i \t185, the ep_r is \t-14966.75, the step is \t2000\n",
      "Ep_i \t190, the ep_r is \t-10838.60, the step is \t2000\n",
      "Ep_i \t195, the ep_r is \t-6505.29, the step is \t2000\n",
      "Ep_i \t200, the ep_r is \t-7258.87, the step is \t2000\n",
      "Ep_i \t205, the ep_r is \t-9761.86, the step is \t2000\n",
      "Ep_i \t210, the ep_r is \t-6907.95, the step is \t2000\n",
      "Ep_i \t215, the ep_r is \t-9742.71, the step is \t2000\n",
      "Ep_i \t220, the ep_r is \t-5842.58, the step is \t2000\n",
      "Ep_i \t225, the ep_r is \t-9454.72, the step is \t2000\n",
      "Ep_i \t230, the ep_r is \t-9900.92, the step is \t2000\n",
      "Ep_i \t235, the ep_r is \t-10098.12, the step is \t2000\n",
      "Ep_i \t240, the ep_r is \t-9430.43, the step is \t2000\n",
      "Ep_i \t245, the ep_r is \t-7062.19, the step is \t2000\n",
      "Ep_i \t250, the ep_r is \t-10851.18, the step is \t2000\n",
      "Ep_i \t255, the ep_r is \t-7976.51, the step is \t2000\n",
      "Ep_i \t260, the ep_r is \t-7828.20, the step is \t2000\n",
      "Ep_i \t265, the ep_r is \t-7423.24, the step is \t2000\n",
      "Ep_i \t270, the ep_r is \t-6627.14, the step is \t2000\n",
      "Ep_i \t275, the ep_r is \t-7416.88, the step is \t2000\n",
      "Ep_i \t280, the ep_r is \t-5565.88, the step is \t2000\n",
      "Ep_i \t285, the ep_r is \t-6073.98, the step is \t2000\n",
      "Ep_i \t290, the ep_r is \t-5992.13, the step is \t2000\n",
      "Ep_i \t295, the ep_r is \t-5816.01, the step is \t2000\n",
      "Ep_i \t300, the ep_r is \t-5451.02, the step is \t2000\n",
      "Ep_i \t305, the ep_r is \t-4723.35, the step is \t2000\n",
      "Ep_i \t310, the ep_r is \t-4855.37, the step is \t2000\n",
      "Ep_i \t315, the ep_r is \t-4213.50, the step is \t2000\n",
      "Ep_i \t320, the ep_r is \t-3529.48, the step is \t2000\n",
      "Ep_i \t325, the ep_r is \t-2509.54, the step is \t2000\n",
      "Ep_i \t330, the ep_r is \t-4170.79, the step is \t2000\n",
      "Ep_i \t335, the ep_r is \t-2410.06, the step is \t2000\n",
      "Ep_i \t340, the ep_r is \t-2318.97, the step is \t2000\n",
      "Ep_i \t345, the ep_r is \t-1793.90, the step is \t2000\n",
      "Ep_i \t350, the ep_r is \t-4266.26, the step is \t2000\n",
      "Ep_i \t355, the ep_r is \t-296.71, the step is \t2000\n",
      "Ep_i \t360, the ep_r is \t-7990.84, the step is \t2000\n",
      "Ep_i \t365, the ep_r is \t-147.17, the step is \t2000\n",
      "Ep_i \t370, the ep_r is \t-7863.36, the step is \t2000\n",
      "Ep_i \t375, the ep_r is \t-126.74, the step is \t2000\n",
      "Ep_i \t380, the ep_r is \t-249.25, the step is \t2000\n",
      "Ep_i \t385, the ep_r is \t-425.65, the step is \t2000\n",
      "Ep_i \t390, the ep_r is \t-125.35, the step is \t2000\n",
      "Ep_i \t395, the ep_r is \t-126.35, the step is \t2000\n",
      "Ep_i \t400, the ep_r is \t-141.61, the step is \t2000\n",
      "Ep_i \t405, the ep_r is \t-235.53, the step is \t2000\n",
      "Ep_i \t410, the ep_r is \t-341.47, the step is \t2000\n",
      "Ep_i \t415, the ep_r is \t-225.87, the step is \t2000\n",
      "Ep_i \t420, the ep_r is \t-356.47, the step is \t2000\n",
      "Ep_i \t425, the ep_r is \t-344.15, the step is \t2000\n",
      "Ep_i \t430, the ep_r is \t-120.95, the step is \t2000\n",
      "Ep_i \t435, the ep_r is \t-131.56, the step is \t2000\n",
      "Ep_i \t440, the ep_r is \t-137.83, the step is \t2000\n",
      "Ep_i \t445, the ep_r is \t-9625.39, the step is \t2000\n",
      "Ep_i \t450, the ep_r is \t-253.23, the step is \t2000\n",
      "Ep_i \t455, the ep_r is \t-481.04, the step is \t2000\n",
      "Ep_i \t460, the ep_r is \t-566.31, the step is \t2000\n",
      "Ep_i \t465, the ep_r is \t-245.54, the step is \t2000\n",
      "Ep_i \t470, the ep_r is \t-124.45, the step is \t2000\n",
      "Ep_i \t475, the ep_r is \t-127.89, the step is \t2000\n",
      "Ep_i \t480, the ep_r is \t-449.07, the step is \t2000\n",
      "Ep_i \t485, the ep_r is \t-124.86, the step is \t2000\n",
      "Ep_i \t490, the ep_r is \t-121.76, the step is \t2000\n",
      "Ep_i \t495, the ep_r is \t-352.24, the step is \t2000\n",
      "Ep_i \t500, the ep_r is \t-1.50, the step is \t2000\n",
      "Ep_i \t505, the ep_r is \t-241.37, the step is \t2000\n",
      "Ep_i \t510, the ep_r is \t-130.70, the step is \t2000\n",
      "Ep_i \t515, the ep_r is \t-399.97, the step is \t2000\n",
      "Ep_i \t520, the ep_r is \t-242.23, the step is \t2000\n",
      "Ep_i \t525, the ep_r is \t-439.77, the step is \t2000\n",
      "Ep_i \t530, the ep_r is \t-130.74, the step is \t2000\n",
      "Ep_i \t535, the ep_r is \t-116.80, the step is \t2000\n",
      "Ep_i \t540, the ep_r is \t-545.22, the step is \t2000\n",
      "Ep_i \t545, the ep_r is \t-123.40, the step is \t2000\n",
      "Ep_i \t550, the ep_r is \t-134.89, the step is \t2000\n",
      "Ep_i \t555, the ep_r is \t-122.19, the step is \t2000\n",
      "Ep_i \t560, the ep_r is \t-128.94, the step is \t2000\n",
      "Ep_i \t565, the ep_r is \t-121.41, the step is \t2000\n",
      "Ep_i \t570, the ep_r is \t-119.44, the step is \t2000\n",
      "Ep_i \t575, the ep_r is \t-19146.01, the step is \t2000\n",
      "Ep_i \t580, the ep_r is \t-332.29, the step is \t2000\n",
      "Ep_i \t585, the ep_r is \t-1133.36, the step is \t2000\n",
      "Ep_i \t590, the ep_r is \t-331.61, the step is \t2000\n",
      "Ep_i \t595, the ep_r is \t-122.37, the step is \t2000\n",
      "Ep_i \t600, the ep_r is \t-134.69, the step is \t2000\n",
      "Ep_i \t605, the ep_r is \t-225.06, the step is \t2000\n",
      "Ep_i \t610, the ep_r is \t-12373.78, the step is \t2000\n",
      "Ep_i \t615, the ep_r is \t-275.45, the step is \t2000\n",
      "Ep_i \t620, the ep_r is \t-164.98, the step is \t2000\n",
      "Ep_i \t625, the ep_r is \t-50.38, the step is \t2000\n",
      "Ep_i \t630, the ep_r is \t-48.66, the step is \t2000\n",
      "Ep_i \t635, the ep_r is \t-38.20, the step is \t2000\n",
      "Ep_i \t640, the ep_r is \t-25.50, the step is \t2000\n",
      "Ep_i \t645, the ep_r is \t-19.64, the step is \t2000\n",
      "Ep_i \t650, the ep_r is \t-254.98, the step is \t2000\n",
      "Ep_i \t655, the ep_r is \t-2.65, the step is \t2000\n",
      "Ep_i \t660, the ep_r is \t-12.54, the step is \t2000\n",
      "Ep_i \t665, the ep_r is \t-179.80, the step is \t2000\n",
      "Ep_i \t670, the ep_r is \t-547.54, the step is \t2000\n",
      "Ep_i \t675, the ep_r is \t-135.92, the step is \t2000\n",
      "Ep_i \t680, the ep_r is \t-854.58, the step is \t2000\n",
      "Ep_i \t685, the ep_r is \t-127.23, the step is \t2000\n",
      "Ep_i \t690, the ep_r is \t-19421.98, the step is \t2000\n",
      "Ep_i \t695, the ep_r is \t-124.36, the step is \t2000\n",
      "Ep_i \t700, the ep_r is \t-10.79, the step is \t2000\n",
      "Ep_i \t705, the ep_r is \t-18056.80, the step is \t2000\n",
      "Ep_i \t710, the ep_r is \t-120.95, the step is \t2000\n",
      "Ep_i \t715, the ep_r is \t-124.70, the step is \t2000\n",
      "Ep_i \t720, the ep_r is \t-17577.31, the step is \t2000\n",
      "Ep_i \t725, the ep_r is \t-308.48, the step is \t2000\n",
      "Ep_i \t730, the ep_r is \t-120.77, the step is \t2000\n",
      "Ep_i \t735, the ep_r is \t-130.19, the step is \t2000\n",
      "Ep_i \t740, the ep_r is \t-235.97, the step is \t2000\n",
      "Ep_i \t745, the ep_r is \t-439.75, the step is \t2000\n",
      "Ep_i \t750, the ep_r is \t-0.40, the step is \t2000\n",
      "Ep_i \t755, the ep_r is \t-125.85, the step is \t2000\n",
      "Ep_i \t760, the ep_r is \t-16465.76, the step is \t2000\n",
      "Ep_i \t765, the ep_r is \t-16356.06, the step is \t2000\n",
      "Ep_i \t770, the ep_r is \t-4.75, the step is \t2000\n",
      "Ep_i \t775, the ep_r is \t-4.88, the step is \t2000\n",
      "Ep_i \t780, the ep_r is \t-15799.17, the step is \t2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep_i \t785, the ep_r is \t-15685.01, the step is \t2000\n",
      "Ep_i \t790, the ep_r is \t-15550.21, the step is \t2000\n",
      "Ep_i \t795, the ep_r is \t-127.95, the step is \t2000\n",
      "Ep_i \t800, the ep_r is \t-230.00, the step is \t2000\n",
      "Ep_i \t805, the ep_r is \t-15132.14, the step is \t2000\n",
      "Ep_i \t810, the ep_r is \t-485.54, the step is \t2000\n",
      "Ep_i \t815, the ep_r is \t-15080.91, the step is \t2000\n",
      "Ep_i \t820, the ep_r is \t-126.36, the step is \t2000\n",
      "Ep_i \t825, the ep_r is \t-120.82, the step is \t2000\n",
      "Ep_i \t830, the ep_r is \t-15069.48, the step is \t2000\n",
      "Ep_i \t835, the ep_r is \t-15084.35, the step is \t2000\n",
      "Ep_i \t840, the ep_r is \t-122.90, the step is \t2000\n",
      "Ep_i \t845, the ep_r is \t-124.83, the step is \t2000\n",
      "Ep_i \t850, the ep_r is \t-235.47, the step is \t2000\n",
      "Ep_i \t855, the ep_r is \t-123.36, the step is \t2000\n",
      "Ep_i \t860, the ep_r is \t-230.44, the step is \t2000\n",
      "Ep_i \t865, the ep_r is \t-245.62, the step is \t2000\n",
      "Ep_i \t870, the ep_r is \t-124.38, the step is \t2000\n",
      "Ep_i \t875, the ep_r is \t-15119.53, the step is \t2000\n",
      "Ep_i \t880, the ep_r is \t-0.43, the step is \t2000\n",
      "Ep_i \t885, the ep_r is \t-132.00, the step is \t2000\n",
      "Ep_i \t890, the ep_r is \t-15115.87, the step is \t2000\n",
      "Ep_i \t895, the ep_r is \t-648.98, the step is \t2000\n",
      "Ep_i \t900, the ep_r is \t-15219.58, the step is \t2000\n",
      "Ep_i \t905, the ep_r is \t-540.77, the step is \t2000\n",
      "Ep_i \t910, the ep_r is \t-15200.17, the step is \t2000\n",
      "Ep_i \t915, the ep_r is \t-134.45, the step is \t2000\n",
      "Ep_i \t920, the ep_r is \t-7.61, the step is \t2000\n",
      "Ep_i \t925, the ep_r is \t-15126.22, the step is \t2000\n",
      "Ep_i \t930, the ep_r is \t-124.62, the step is \t2000\n",
      "Ep_i \t935, the ep_r is \t-15057.57, the step is \t2000\n",
      "Ep_i \t940, the ep_r is \t-1778.98, the step is \t2000\n",
      "Ep_i \t945, the ep_r is \t-1198.73, the step is \t2000\n",
      "Ep_i \t950, the ep_r is \t-125.50, the step is \t2000\n",
      "Ep_i \t955, the ep_r is \t-1181.27, the step is \t2000\n",
      "Ep_i \t960, the ep_r is \t-1050.15, the step is \t2000\n",
      "Ep_i \t965, the ep_r is \t-124.17, the step is \t2000\n",
      "Ep_i \t970, the ep_r is \t-0.56, the step is \t2000\n",
      "Ep_i \t975, the ep_r is \t-842.99, the step is \t2000\n",
      "Ep_i \t980, the ep_r is \t-1161.63, the step is \t2000\n",
      "Ep_i \t985, the ep_r is \t-541.03, the step is \t2000\n",
      "Ep_i \t990, the ep_r is \t-9.12, the step is \t2000\n",
      "Ep_i \t995, the ep_r is \t-233.59, the step is \t2000\n",
      "Ep_i \t1000, the ep_r is \t-2779.74, the step is \t2000\n",
      "Ep_i \t1005, the ep_r is \t-125.57, the step is \t2000\n",
      "Ep_i \t1010, the ep_r is \t-757.51, the step is \t2000\n",
      "Ep_i \t1015, the ep_r is \t-5.60, the step is \t2000\n",
      "Ep_i \t1020, the ep_r is \t-121.70, the step is \t2000\n",
      "Ep_i \t1025, the ep_r is \t-14105.62, the step is \t2000\n",
      "Ep_i \t1030, the ep_r is \t-131.47, the step is \t2000\n",
      "Ep_i \t1035, the ep_r is \t-348.41, the step is \t2000\n",
      "Ep_i \t1040, the ep_r is \t-561.00, the step is \t2000\n",
      "Ep_i \t1045, the ep_r is \t-119.46, the step is \t2000\n",
      "Ep_i \t1050, the ep_r is \t-125.96, the step is \t2000\n",
      "Ep_i \t1055, the ep_r is \t-118.30, the step is \t2000\n",
      "Ep_i \t1060, the ep_r is \t-654.16, the step is \t2000\n",
      "Ep_i \t1065, the ep_r is \t-0.29, the step is \t2000\n",
      "Ep_i \t1070, the ep_r is \t-116.42, the step is \t2000\n",
      "Ep_i \t1075, the ep_r is \t-227.77, the step is \t2000\n",
      "Ep_i \t1080, the ep_r is \t-224.76, the step is \t2000\n",
      "Ep_i \t1085, the ep_r is \t-1.37, the step is \t2000\n",
      "Ep_i \t1090, the ep_r is \t-122.67, the step is \t2000\n",
      "Ep_i \t1095, the ep_r is \t-2.15, the step is \t2000\n",
      "Ep_i \t1100, the ep_r is \t-651.42, the step is \t2000\n",
      "Ep_i \t1105, the ep_r is \t-127.07, the step is \t2000\n",
      "Ep_i \t1110, the ep_r is \t-442.00, the step is \t2000\n",
      "Ep_i \t1115, the ep_r is \t-843.80, the step is \t2000\n",
      "Ep_i \t1120, the ep_r is \t-441.85, the step is \t2000\n",
      "Ep_i \t1125, the ep_r is \t-229.26, the step is \t2000\n",
      "Ep_i \t1130, the ep_r is \t-537.12, the step is \t2000\n",
      "Ep_i \t1135, the ep_r is \t-123.86, the step is \t2000\n",
      "Ep_i \t1140, the ep_r is \t-128.41, the step is \t2000\n",
      "Ep_i \t1145, the ep_r is \t-5.14, the step is \t2000\n",
      "Ep_i \t1150, the ep_r is \t-125.46, the step is \t2000\n",
      "Ep_i \t1155, the ep_r is \t-127.65, the step is \t2000\n",
      "Ep_i \t1160, the ep_r is \t-25.97, the step is \t2000\n",
      "Ep_i \t1165, the ep_r is \t-802.46, the step is \t2000\n",
      "Ep_i \t1170, the ep_r is \t-356.63, the step is \t2000\n",
      "Ep_i \t1175, the ep_r is \t-153.39, the step is \t2000\n",
      "Ep_i \t1180, the ep_r is \t-36.49, the step is \t2000\n",
      "Ep_i \t1185, the ep_r is \t-1047.60, the step is \t2000\n",
      "Ep_i \t1190, the ep_r is \t-176.34, the step is \t2000\n",
      "Ep_i \t1195, the ep_r is \t-167.32, the step is \t2000\n",
      "Ep_i \t1200, the ep_r is \t-179.28, the step is \t2000\n",
      "Ep_i \t1205, the ep_r is \t-278.82, the step is \t2000\n",
      "Ep_i \t1210, the ep_r is \t-417.18, the step is \t2000\n",
      "Ep_i \t1215, the ep_r is \t-41.21, the step is \t2000\n",
      "Ep_i \t1220, the ep_r is \t-279.13, the step is \t2000\n",
      "Ep_i \t1225, the ep_r is \t-376.62, the step is \t2000\n",
      "Ep_i \t1230, the ep_r is \t-806.35, the step is \t2000\n",
      "Ep_i \t1235, the ep_r is \t-379.66, the step is \t2000\n",
      "Ep_i \t1240, the ep_r is \t-12106.41, the step is \t2000\n",
      "Ep_i \t1245, the ep_r is \t-446.61, the step is \t2000\n",
      "Ep_i \t1250, the ep_r is \t-261.49, the step is \t2000\n",
      "Ep_i \t1255, the ep_r is \t-130.79, the step is \t2000\n",
      "Ep_i \t1260, the ep_r is \t-122.94, the step is \t2000\n",
      "Ep_i \t1265, the ep_r is \t-121.29, the step is \t2000\n",
      "Ep_i \t1270, the ep_r is \t-430.39, the step is \t2000\n",
      "Ep_i \t1275, the ep_r is \t-334.08, the step is \t2000\n",
      "Ep_i \t1280, the ep_r is \t-585.52, the step is \t2000\n",
      "Ep_i \t1285, the ep_r is \t-140.02, the step is \t2000\n",
      "Ep_i \t1290, the ep_r is \t-8.00, the step is \t2000\n",
      "Ep_i \t1295, the ep_r is \t-7.44, the step is \t2000\n",
      "Ep_i \t1300, the ep_r is \t-122.71, the step is \t2000\n",
      "Ep_i \t1305, the ep_r is \t-5389.21, the step is \t2000\n",
      "Ep_i \t1310, the ep_r is \t-304.84, the step is \t2000\n",
      "Ep_i \t1315, the ep_r is \t-244.97, the step is \t2000\n",
      "Ep_i \t1320, the ep_r is \t-245.14, the step is \t2000\n",
      "Ep_i \t1325, the ep_r is \t-125.27, the step is \t2000\n",
      "Ep_i \t1330, the ep_r is \t-128.88, the step is \t2000\n",
      "Ep_i \t1335, the ep_r is \t-6.03, the step is \t2000\n",
      "Ep_i \t1340, the ep_r is \t-128.32, the step is \t2000\n",
      "Ep_i \t1345, the ep_r is \t-120.74, the step is \t2000\n",
      "Ep_i \t1350, the ep_r is \t-3.63, the step is \t2000\n",
      "Ep_i \t1355, the ep_r is \t-131.86, the step is \t2000\n",
      "Ep_i \t1360, the ep_r is \t-545.96, the step is \t2000\n",
      "Ep_i \t1365, the ep_r is \t-137.87, the step is \t2000\n",
      "Ep_i \t1370, the ep_r is \t-13571.87, the step is \t2000\n",
      "Ep_i \t1375, the ep_r is \t-448.19, the step is \t2000\n",
      "Ep_i \t1380, the ep_r is \t-341.35, the step is \t2000\n",
      "Ep_i \t1385, the ep_r is \t-348.57, the step is \t2000\n",
      "Ep_i \t1390, the ep_r is \t-127.56, the step is \t2000\n",
      "Ep_i \t1395, the ep_r is \t-137.10, the step is \t2000\n",
      "Ep_i \t1400, the ep_r is \t-2164.83, the step is \t2000\n",
      "Ep_i \t1405, the ep_r is \t-340.87, the step is \t2000\n",
      "Ep_i \t1410, the ep_r is \t-845.26, the step is \t2000\n",
      "Ep_i \t1415, the ep_r is \t-177.73, the step is \t2000\n",
      "Ep_i \t1420, the ep_r is \t-668.83, the step is \t2000\n",
      "Ep_i \t1425, the ep_r is \t-96.87, the step is \t2000\n",
      "Ep_i \t1430, the ep_r is \t-271.26, the step is \t2000\n",
      "Ep_i \t1435, the ep_r is \t-348.14, the step is \t2000\n",
      "Ep_i \t1440, the ep_r is \t-522.72, the step is \t2000\n",
      "Ep_i \t1445, the ep_r is \t-13.62, the step is \t2000\n",
      "Ep_i \t1450, the ep_r is \t-235.06, the step is \t2000\n",
      "Ep_i \t1455, the ep_r is \t-230.54, the step is \t2000\n",
      "Ep_i \t1460, the ep_r is \t-128.41, the step is \t2000\n",
      "Ep_i \t1465, the ep_r is \t-350.55, the step is \t2000\n",
      "Ep_i \t1470, the ep_r is \t-123.34, the step is \t2000\n",
      "Ep_i \t1475, the ep_r is \t-120.93, the step is \t2000\n",
      "Ep_i \t1480, the ep_r is \t-533.98, the step is \t2000\n",
      "Ep_i \t1485, the ep_r is \t-125.30, the step is \t2000\n",
      "Ep_i \t1490, the ep_r is \t-125.94, the step is \t2000\n",
      "Ep_i \t1495, the ep_r is \t-431.00, the step is \t2000\n"
     ]
    }
   ],
   "source": [
    "agent = DDPG(state_dim, action_dim, max_action)\n",
    "ep_r = 0\n",
    "print(\"====================================\")\n",
    "print(\"Collection Experience...\")\n",
    "print(\"====================================\")\n",
    "if LOAD:\n",
    "    agent.load()\n",
    "for i in range(MAX_EPISODE):\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        action = agent.select_action(state)\n",
    "        action = (action + np.random.normal(0, EXPLORATION_NOISE, size=env.action_space.shape[0])).clip(\n",
    "            env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        ep_r += reward\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        agent.replay_buffer.push((state, next_state, action, reward, np.float(done)))\n",
    "        state = next_state\n",
    "        if done or t >= MAX_LENGTH_OF_TRAJECTORY:\n",
    "            agent.writer.add_scalar('ep_r', ep_r, global_step=i)\n",
    "            if i % PRINT_LOG == 0:\n",
    "                print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
    "            ep_r = 0\n",
    "            break\n",
    "\n",
    "    if i % LOG_INTERVAL == 0:\n",
    "        agent.save()\n",
    "    if len(agent.replay_buffer.storage) >= CAPACITY-1:\n",
    "        agent.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ITERATION = 10\n",
    "MAX_LENGTH_OF_TRAJECTORY = 2000 # num of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "model has been loaded...\n",
      "====================================\n",
      "Ep_i \t0, the ep_r is \t-10.99, the step is \t2000\n",
      "Ep_i \t1, the ep_r is \t-508.22, the step is \t2000\n",
      "Ep_i \t2, the ep_r is \t-334.91, the step is \t2000\n",
      "Ep_i \t3, the ep_r is \t-246.38, the step is \t2000\n",
      "Ep_i \t4, the ep_r is \t-604.13, the step is \t2000\n",
      "Ep_i \t5, the ep_r is \t-130.22, the step is \t2000\n",
      "Ep_i \t6, the ep_r is \t-441.71, the step is \t2000\n",
      "Ep_i \t7, the ep_r is \t-129.36, the step is \t2000\n",
      "Ep_i \t8, the ep_r is \t-133.75, the step is \t2000\n",
      "Ep_i \t9, the ep_r is \t-132.05, the step is \t2000\n"
     ]
    }
   ],
   "source": [
    "agent = DDPG(state_dim, action_dim, max_action)\n",
    "ep_r = 0\n",
    "agent.load()\n",
    "for i in range(TEST_ITERATION):\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, info = env.step(np.float32(action))\n",
    "        ep_r += reward\n",
    "        env.render()\n",
    "        if done or t >= MAX_LENGTH_OF_TRAJECTORY:\n",
    "            print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
    "            ep_r = 0\n",
    "            break\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
